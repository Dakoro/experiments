{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10\n",
      "Kernel Execution Time: 0.143615s | Block Size: 32 | Grid Size: (2, 2)\n",
      "New best kernel found by agent 0 with time 0.143615s\n",
      "Kernel Execution Time: 0.000145s | Block Size: 32 | Grid Size: (2, 2)\n",
      "New best kernel found by agent 1 with time 0.000145s\n",
      "Kernel Execution Time: 0.000026s | Block Size: 8 | Grid Size: (8, 8)\n",
      "New best kernel found by agent 2 with time 0.000026s\n",
      "Kernel Execution Time: 0.054827s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.052365s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Iteration 2/10\n",
      "Kernel Execution Time: 0.000025s | Block Size: 32 | Grid Size: (2, 2)\n",
      "New best kernel found by agent 0 with time 0.000025s\n",
      "Kernel Execution Time: 0.000018s | Block Size: 16 | Grid Size: (4, 4)\n",
      "New best kernel found by agent 1 with time 0.000018s\n",
      "Kernel Execution Time: 0.000014s | Block Size: 8 | Grid Size: (8, 8)\n",
      "New best kernel found by agent 2 with time 0.000014s\n",
      "Kernel Execution Time: 0.000013s | Block Size: 8 | Grid Size: (8, 8)\n",
      "New best kernel found by agent 3 with time 0.000013s\n",
      "Kernel Execution Time: 0.000013s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Iteration 3/10\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "New best kernel found by agent 0 with time 0.000012s\n",
      "Kernel Execution Time: 0.000015s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "New best kernel found by agent 2 with time 0.000012s\n",
      "Kernel Execution Time: 0.000015s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 8 | Grid Size: (8, 8)\n",
      "New best kernel found by agent 4 with time 0.000011s\n",
      "Iteration 4/10\n",
      "Kernel Execution Time: 0.050900s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000287s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000016s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000015s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000489s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Iteration 5/10\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000018s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000016s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000013s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Iteration 6/10\n",
      "Kernel Execution Time: 0.000013s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000013s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000015s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Iteration 7/10\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000014s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000016s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000015s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Iteration 8/10\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000015s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Iteration 9/10\n",
      "Kernel Execution Time: 0.000016s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000014s | Block Size: 32 | Grid Size: (2, 2)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Iteration 10/10\n",
      "Kernel Execution Time: 0.000012s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 8 | Grid Size: (8, 8)\n",
      "Kernel Execution Time: 0.000011s | Block Size: 16 | Grid Size: (4, 4)\n",
      "Kernel Execution Time: 0.000012s | Block Size: 16 | Grid Size: (4, 4)\n",
      "\n",
      "Best Discovered CUDA Kernel:\n",
      "\n",
      "\n",
      "    extern \"C\" __global__\n",
      "    void matmul_kernel(float *A, float *B, float *C, int N) {\n",
      "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "\n",
      "        if(row < N && col < N) {\n",
      "            float sum = 0.0;\n",
      "            for(int k = 0; k < N; k++) {\n",
      "                sum += A[row * N + k] * B[k * N + col];\n",
      "            }\n",
      "            C[row * N + col] = sum;\n",
      "        }\n",
      "    }\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define baseline matrix multiplication using NumPy\n",
    "def baseline_matmul(A, B):\n",
    "    return np.dot(A, B)\n",
    "\n",
    "# CUDA Kernel Templates\n",
    "CUDA_KERNEL_TEMPLATES = [\n",
    "    \"\"\"\n",
    "    extern \"C\" __global__\n",
    "    void matmul_kernel(float *A, float *B, float *C, int N) {\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if(row < N && col < N) {\n",
    "            float sum = 0.0;\n",
    "            for(int k = 0; k < N; k++) {\n",
    "                sum += A[row * N + k] * B[k * N + col];\n",
    "            }\n",
    "            C[row * N + col] = sum;\n",
    "        }\n",
    "    }\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    extern \"C\" __global__\n",
    "    void matmul_kernel(float *A, float *B, float *C, int N) {\n",
    "        __shared__ float As[16][16];\n",
    "        __shared__ float Bs[16][16];\n",
    "\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        float sum = 0.0;\n",
    "\n",
    "        for (int tile = 0; tile < N / 16; ++tile) {\n",
    "            if (row < N && (tile * 16 + threadIdx.x) < N) {\n",
    "                As[threadIdx.y][threadIdx.x] = A[row * N + (tile * 16 + threadIdx.x)];\n",
    "            } else {\n",
    "                As[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "            }\n",
    "            if ((tile * 16 + threadIdx.y) < N && col < N) {\n",
    "                Bs[threadIdx.y][threadIdx.x] = B[(tile * 16 + threadIdx.y) * N + col];\n",
    "            } else {\n",
    "                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "            }\n",
    "            __syncthreads();\n",
    "\n",
    "            for (int k = 0; k < 16; ++k)\n",
    "                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "            __syncthreads();\n",
    "        }\n",
    "        if (row < N && col < N) {\n",
    "            C[row * N + col] = sum;\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Compile CUDA kernels\n",
    "def compile_cuda_kernel(kernel_code):\n",
    "    try:\n",
    "        return cp.RawKernel(kernel_code, \"matmul_kernel\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA Compilation Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Swarm Optimization\n",
    "class SwarmCUDAOptimizer:\n",
    "    def __init__(self, N=64, num_agents=5, max_iters=10):\n",
    "        self.N = N  # Matrix size\n",
    "        self.num_agents = num_agents\n",
    "        self.max_iters = max_iters\n",
    "        self.knowledge_archive = []\n",
    "\n",
    "    def generate_random_kernel(self):\n",
    "        # Select a random kernel template and modify block sizes\n",
    "        base_kernel = random.choice(CUDA_KERNEL_TEMPLATES)\n",
    "        block_size = random.choice([8, 16, 32])\n",
    "        modified_kernel = base_kernel.replace(\"16\", str(block_size))  # Adjust shared memory sizes\n",
    "        return modified_kernel, block_size\n",
    "\n",
    "    def evaluate_kernel(self, kernel_code, block_size, A, B):\n",
    "        try:\n",
    "            kernel = compile_cuda_kernel(kernel_code)\n",
    "            if kernel is None:\n",
    "                return float('inf')  # Compilation failure\n",
    "\n",
    "            C = cp.zeros((self.N, self.N), dtype=cp.float32)\n",
    "            d_A = cp.asarray(A)\n",
    "            d_B = cp.asarray(B)\n",
    "            d_C = cp.asarray(C)\n",
    "            grid_size = ( (self.N + block_size - 1) // block_size, (self.N + block_size - 1) // block_size)  # Correct grid size\n",
    "            block_size_tuple = (block_size, block_size)  # Use a tuple for block size\n",
    "\n",
    "            start = time.time()\n",
    "            kernel(grid_size, block_size_tuple, (d_A, d_B, d_C, self.N))\n",
    "            cp.cuda.Device(0).synchronize()\n",
    "            end = time.time()\n",
    "\n",
    "            execution_time = end - start\n",
    "            print(f\"Kernel Execution Time: {execution_time:.6f}s | Block Size: {block_size} | Grid Size: {grid_size}\")\n",
    "            return execution_time  # Ensure only a single float value is returned\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CUDA Execution Error: {e}\")\n",
    "            return float('inf')  # If kernel fails, assign worst time\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        A = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "        B = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "\n",
    "        best_time = float('inf')\n",
    "        best_kernel = None\n",
    "\n",
    "        for iter in range(self.max_iters):\n",
    "            print(f\"Iteration {iter+1}/{self.max_iters}\")\n",
    "            candidates = []\n",
    "\n",
    "            for agent in range(self.num_agents):\n",
    "                kernel_code, block_size = self.generate_random_kernel()\n",
    "                exec_time = self.evaluate_kernel(kernel_code, block_size, A, B)\n",
    "\n",
    "                # No need to check type, evaluate_kernel handles errors\n",
    "                if exec_time < best_time:\n",
    "                    best_time = exec_time\n",
    "                    best_kernel = kernel_code\n",
    "                    print(f\"New best kernel found by agent {agent} with time {best_time:.6f}s\")\n",
    "\n",
    "                candidates.append((exec_time, kernel_code, block_size))  # Always append\n",
    "\n",
    "            # Swarm-inspired selection: keep top 3\n",
    "            candidates.sort()  # Sorts by the first element (exec_time)\n",
    "            self.knowledge_archive.append(candidates[:3])\n",
    "\n",
    "        return best_kernel\n",
    "\n",
    "# Run the swarm-based optimizer with debugging\n",
    "optimizer = SwarmCUDAOptimizer(N=64, num_agents=5, max_iters=10)\n",
    "best_kernel_code = optimizer.optimize()\n",
    "\n",
    "# Show the best discovered CUDA kernel\n",
    "print(\"\\nBest Discovered CUDA Kernel:\\n\")\n",
    "print(best_kernel_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA optimizer...\n",
      "Generating test data...\n",
      "Data successfully transferred to GPU\n",
      "\n",
      "Starting optimization...\n",
      "Iteration 1/5\n",
      "New best configuration - Block size: 8, Time: 0.049121s\n",
      "New best configuration - Block size: 16, Time: 0.045454s\n",
      "New best configuration - Block size: 32, Time: 0.044713s\n",
      "Iteration 2/5\n",
      "New best configuration - Block size: 8, Time: 0.000018s\n",
      "New best configuration - Block size: 16, Time: 0.000014s\n",
      "Iteration 3/5\n",
      "New best configuration - Block size: 8, Time: 0.000013s\n",
      "New best configuration - Block size: 16, Time: 0.000012s\n",
      "Iteration 4/5\n",
      "New best configuration - Block size: 8, Time: 0.000011s\n",
      "Iteration 5/5\n",
      "New best configuration - Block size: 8, Time: 0.000011s\n",
      "\n",
      "Running NumPy comparison...\n",
      "\n",
      "Final Results:\n",
      "Best CUDA time: 0.000011s\n",
      "NumPy time: 0.008014s\n",
      "Best block size: 8\n",
      "Speedup over NumPy: 712.97x\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "def check_cuda_availability():\n",
    "    \"\"\"Check if CUDA is available and initialize the device\"\"\"\n",
    "    try:\n",
    "        device = cp.cuda.Device(0)  # Get the current device\n",
    "        device.synchronize()  # Ensure device is ready\n",
    "        return True\n",
    "    except cp.cuda.runtime.CUDARuntimeError as e:\n",
    "        print(f\"CUDA Error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:  # Catch other potential exceptions\n",
    "        print(f\"Unexpected CUDA initialization error: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_kernel_template(block_size):\n",
    "    return f\"\"\"\n",
    "    extern \"C\" __global__\n",
    "    void matmul_kernel(const float *A, const float *B, float *C, const int N) {{\n",
    "        const int tx = threadIdx.x;\n",
    "        const int ty = threadIdx.y;\n",
    "        const int bx = blockIdx.x;\n",
    "        const int by = blockIdx.y;\n",
    "\n",
    "        const int row = by * {block_size} + ty;\n",
    "        const int col = bx * {block_size} + tx;\n",
    "\n",
    "        if(row < N && col < N) {{\n",
    "            float sum = 0.0f;\n",
    "            for(int k = 0; k < N; k++) {{\n",
    "                sum += A[row * N + k] * B[k * N + col];\n",
    "            }}\n",
    "            C[row * N + col] = sum;\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "class SwarmCUDAOptimizer:\n",
    "    def __init__(self, N=64):\n",
    "        if not check_cuda_availability():\n",
    "            print(\"CUDA is not available. Exiting.\")\n",
    "            self.N = None  # Set N to None to indicate CUDA failure\n",
    "            return\n",
    "\n",
    "        self.N = N\n",
    "        self.best_kernel = None\n",
    "        self.best_block_size = None\n",
    "        self.best_time = float('inf')\n",
    "\n",
    "        print(\"Generating test data...\")\n",
    "        self.A_host = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "        self.B_host = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "\n",
    "        try:\n",
    "            with cp.cuda.Device(0):\n",
    "                self.A = cp.asarray(self.A_host)\n",
    "                self.B = cp.asarray(self.B_host)\n",
    "                print(\"Data successfully transferred to GPU\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to transfer data to GPU: {e}\")  #Print and return\n",
    "            self.N = None #Set N to none to avoid operations\n",
    "            return\n",
    "\n",
    "    def evaluate_kernel(self, block_size):\n",
    "        try:\n",
    "            kernel_code = generate_kernel_template(block_size)\n",
    "            kernel = cp.RawKernel(kernel_code, 'matmul_kernel')\n",
    "\n",
    "            C = cp.zeros((self.N, self.N), dtype=cp.float32)\n",
    "            grid_dim = (self.N + block_size - 1) // block_size\n",
    "\n",
    "            with cp.cuda.Device(0):\n",
    "                start = time.perf_counter()\n",
    "                kernel((grid_dim, grid_dim), (block_size, block_size), (self.A, self.B, C, self.N))\n",
    "                cp.cuda.Stream.null.synchronize()\n",
    "                end = time.perf_counter()\n",
    "\n",
    "                execution_time = end - start\n",
    "\n",
    "                if cp.any(cp.isnan(C)):\n",
    "                    print(\"Warning: NaN values detected in output\")\n",
    "                    return float('inf')\n",
    "\n",
    "                return execution_time\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating kernel: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def optimize(self):\n",
    "        block_sizes = [8, 16, 32]\n",
    "        iterations = 5\n",
    "\n",
    "        print(\"\\nStarting optimization...\")\n",
    "        for iteration in range(iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{iterations}\")\n",
    "            for block_size in block_sizes:\n",
    "                time = self.evaluate_kernel(block_size)\n",
    "\n",
    "                if time < self.best_time:\n",
    "                    self.best_time = time\n",
    "                    self.best_block_size = block_size\n",
    "                    self.best_kernel = generate_kernel_template(block_size)\n",
    "                    print(f\"New best configuration - Block size: {block_size}, Time: {time:.6f}s\")\n",
    "\n",
    "        return self.best_kernel, self.best_block_size, self.best_time\n",
    "\n",
    "# Run optimization with error handling\n",
    "try:\n",
    "    print(\"Initializing CUDA optimizer...\")\n",
    "    optimizer = SwarmCUDAOptimizer(N=64)\n",
    "    if optimizer.N is None:  # Check if CUDA initialization failed\n",
    "        exit()\n",
    "    best_kernel, best_block_size, best_time = optimizer.optimize()\n",
    "\n",
    "    # Compare with NumPy baseline\n",
    "    if best_kernel:\n",
    "        print(\"\\nRunning NumPy comparison...\")\n",
    "        start_time = time.time()\n",
    "        np.dot(optimizer.A_host, optimizer.B_host)  # Use A_host and B_host for NumPy\n",
    "        numpy_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(f\"Best CUDA time: {best_time:.6f}s\")\n",
    "        print(f\"NumPy time: {numpy_time:.6f}s\")\n",
    "        print(f\"Best block size: {best_block_size}\")\n",
    "        if best_time < numpy_time:\n",
    "            print(f\"Speedup over NumPy: {numpy_time/best_time:.2f}x\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during optimization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA optimizer with Fish Swarm Dynamics...\n",
      "Generating test data...\n",
      "Data successfully transferred to GPU\n",
      "\n",
      "Starting Fish Swarm Optimization...\n",
      "Iteration 1/10\n",
      "New best - Block size: 8, Time: 0.000043s\n",
      "New best - Block size: 16, Time: 0.000017s\n",
      "Iteration 2/10\n",
      "New best - Block size: 8, Time: 0.000014s\n",
      "Iteration 3/10\n",
      "Iteration 4/10\n",
      "Iteration 5/10\n",
      "New best - Block size: 13, Time: 0.000012s\n",
      "Iteration 6/10\n",
      "Iteration 7/10\n",
      "Iteration 8/10\n",
      "Iteration 9/10\n",
      "Iteration 10/10\n",
      "\n",
      "Running NumPy comparison...\n",
      "\n",
      "Final Results:\n",
      "Best CUDA time: 0.000012s\n",
      "NumPy time: 0.000026s\n",
      "Best block size: 13\n",
      "Speedup over NumPy: 2.16x\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "def check_cuda_availability():\n",
    "    \"\"\"Check if CUDA is available and initialize the device\"\"\"\n",
    "    try:\n",
    "        device = cp.cuda.Device(0)  # Get the current device\n",
    "        device.synchronize()  # Ensure device is ready\n",
    "        return True\n",
    "    except cp.cuda.runtime.CUDARuntimeError as e:\n",
    "        print(f\"CUDA Error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected CUDA initialization error: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_kernel_template(block_size):\n",
    "    return f\"\"\"\n",
    "    extern \"C\" __global__\n",
    "    void matmul_kernel(const float *A, const float *B, float *C, const int N) {{\n",
    "        const int tx = threadIdx.x;\n",
    "        const int ty = threadIdx.y;\n",
    "        const int bx = blockIdx.x;\n",
    "        const int by = blockIdx.y;\n",
    "\n",
    "        const int row = by * {block_size} + ty;\n",
    "        const int col = bx * {block_size} + tx;\n",
    "\n",
    "        if(row < N && col < N) {{\n",
    "            float sum = 0.0f;\n",
    "            for(int k = 0; k < N; k++) {{\n",
    "                sum += A[row * N + k] * B[k * N + col];\n",
    "            }}\n",
    "            C[row * N + col] = sum;\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "class SwarmCUDAFishOptimizer:\n",
    "    def __init__(self, N=64):\n",
    "        if not check_cuda_availability():\n",
    "            print(\"CUDA is not available. Exiting.\")\n",
    "            self.N = None\n",
    "            return\n",
    "\n",
    "        self.N = N\n",
    "        self.best_kernel = None\n",
    "        self.best_block_size = None\n",
    "        self.best_time = float('inf')\n",
    "\n",
    "        print(\"Generating test data...\")\n",
    "        self.A_host = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "        self.B_host = np.random.rand(self.N, self.N).astype(np.float32)\n",
    "\n",
    "        try:\n",
    "            with cp.cuda.Device(0):\n",
    "                self.A = cp.asarray(self.A_host)\n",
    "                self.B = cp.asarray(self.B_host)\n",
    "                print(\"Data successfully transferred to GPU\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to transfer data to GPU: {e}\")\n",
    "            self.N = None\n",
    "            return\n",
    "\n",
    "    def evaluate_kernel(self, block_size):\n",
    "        try:\n",
    "            kernel_code = generate_kernel_template(block_size)\n",
    "            kernel = cp.RawKernel(kernel_code, 'matmul_kernel')\n",
    "\n",
    "            C = cp.zeros((self.N, self.N), dtype=cp.float32)\n",
    "            grid_dim = (self.N + block_size - 1) // block_size\n",
    "\n",
    "            with cp.cuda.Device(0):\n",
    "                start = time.perf_counter()\n",
    "                kernel((grid_dim, grid_dim), (block_size, block_size), (self.A, self.B, C, self.N))\n",
    "                cp.cuda.Stream.null.synchronize()\n",
    "                end = time.perf_counter()\n",
    "\n",
    "                execution_time = end - start\n",
    "\n",
    "                if cp.any(cp.isnan(C)):\n",
    "                    print(\"Warning: NaN values detected in output\")\n",
    "                    return float('inf')\n",
    "\n",
    "                return execution_time\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating kernel: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def viscosity(self, time):\n",
    "        \"\"\"Define viscosity as an inverse function of execution time\"\"\"\n",
    "        return 1 / (time + 1e-6)\n",
    "\n",
    "    def optimize(self):\n",
    "        block_sizes = [8, 16, 32]\n",
    "        fish_population = [{\"size\": b, \"velocity\": 0.1, \"time\": float('inf')} for b in block_sizes]\n",
    "        iterations = 10\n",
    "        max_jump = 8  # Maximum random movement per iteration\n",
    "\n",
    "        print(\"\\nStarting Fish Swarm Optimization...\")\n",
    "        for iteration in range(iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{iterations}\")\n",
    "\n",
    "            for fish in fish_population:\n",
    "                block_size = fish[\"size\"]\n",
    "                execution_time = self.evaluate_kernel(block_size)\n",
    "\n",
    "                # Update best solution\n",
    "                if execution_time < self.best_time:\n",
    "                    self.best_time = execution_time\n",
    "                    self.best_block_size = block_size\n",
    "                    self.best_kernel = generate_kernel_template(block_size)\n",
    "                    print(f\"New best - Block size: {block_size}, Time: {execution_time:.6f}s\")\n",
    "\n",
    "                # Compute viscosity-based movement\n",
    "                fish[\"time\"] = execution_time\n",
    "                fish[\"velocity\"] = self.viscosity(execution_time)\n",
    "\n",
    "            # Apply fluid-based movement\n",
    "            avg_time = sum(f[\"time\"] for f in fish_population) / len(fish_population)\n",
    "            for fish in fish_population:\n",
    "                viscosity_factor = self.viscosity(fish[\"time\"]) / self.viscosity(avg_time)\n",
    "\n",
    "                # Fish move toward faster execution zones\n",
    "                if random.random() < viscosity_factor:\n",
    "                    fish[\"size\"] = max(8, min(32, fish[\"size\"] + random.randint(-max_jump, max_jump)))\n",
    "\n",
    "        return self.best_kernel, self.best_block_size, self.best_time\n",
    "\n",
    "# Run Fish Swarm Optimization with error handling\n",
    "try:\n",
    "    print(\"Initializing CUDA optimizer with Fish Swarm Dynamics...\")\n",
    "    optimizer = SwarmCUDAFishOptimizer(N=64)\n",
    "    if optimizer.N is None:\n",
    "        exit()\n",
    "    best_kernel, best_block_size, best_time = optimizer.optimize()\n",
    "\n",
    "    # Compare with NumPy baseline\n",
    "    if best_kernel:\n",
    "        print(\"\\nRunning NumPy comparison...\")\n",
    "        start_time = time.time()\n",
    "        np.dot(optimizer.A_host, optimizer.B_host)\n",
    "        numpy_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(f\"Best CUDA time: {best_time:.6f}s\")\n",
    "        print(f\"NumPy time: {numpy_time:.6f}s\")\n",
    "        print(f\"Best block size: {best_block_size}\")\n",
    "        if best_time < numpy_time:\n",
    "            print(f\"Speedup over NumPy: {numpy_time/best_time:.2f}x\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during optimization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
